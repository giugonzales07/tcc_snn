{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd68cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "#import kagglehub\n",
    "#from kagglehub import KaggleDatasetAdapter\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "import cv2 \n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "import argparse\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1efc0a2",
   "metadata": {},
   "source": [
    "# Data over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Load the data(images)\n",
    "x_npz = np.load(\"../../data/x_images_arrays.npz\")\n",
    "X = x_npz[\"arr_0\"]\n",
    "y_npz = np.load(\"../../data/y_labels_arrays.npz\")\n",
    "Y = y_npz[\"arr_0\"]\n",
    "\n",
    "print(np.array(X).shape)\n",
    "print(np.array(Y).shape)\n",
    "\n",
    "# Separete the data into train and test sets\n",
    "# 80% train, 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=1, stratify=Y)\n",
    "\n",
    "#x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "print(np.array(x_train).shape)\n",
    "#print(np.array(x_val).shape)\n",
    "print(np.array(x_test).shape)\n",
    "# (3045, 128, 128, 3)\n",
    "# (381, 128, 128, 3)\n",
    "# (381, 128, 128, 3)\n",
    "\n",
    "# Transform the data to tensor\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            #transforms.Resize((28, 28)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# Apply the transform to the data\n",
    "dataset_train = CustomImageDataset(x_train, y_train, transform=transform)\n",
    "#dataset_val = CustomImageDataset(x_val, y_val, transform=transform)\n",
    "dataset_test = CustomImageDataset(x_test, y_test, transform=transform)\n",
    "\n",
    "img, label = dataset_train[0]\n",
    "print(img.shape, label) # [grayscale=1, size=128, size=128] label=0 ('Nothing')\n",
    "print(dataset_train.__len__()) # 3045 images\n",
    "#print(dataset_val.__len__()) # 381 images\n",
    "print(dataset_test.__len__()) # 381 images\n",
    "\n",
    "# Load into the DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "import numpy as np\n",
    "from oversampling.o2pf import O2PF\n",
    "from common.common import COMMON\n",
    "import sys\n",
    "\n",
    "if len(sys.argv) <= 2:\n",
    "\tprint('Usage: %s <train dataset>  <k_max>' % sys.argv[0])\n",
    "\traise SystemExit\n",
    "\n",
    "train = np.loadtxt(sys.argv[1],delimiter=',', dtype=np.float32)\n",
    "\n",
    "X = train[:,:-1]\n",
    "y = train[:,-1].astype(int)\n",
    "\n",
    "o2pf = O2PF(k_max=int(sys.argv[2]))\n",
    "X_res, y_res = o2pf.fit_resample(X, y)\n",
    "\n",
    "path = 'data'\n",
    "common = COMMON()\n",
    "common.saveDataset(X_res, y_res, path, o2pf.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78764002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling\n",
    "import numpy as np\n",
    "from undersampling.opf_us import OpfUS\n",
    "from common.common import COMMON\n",
    "import sys\n",
    "\n",
    "if len(sys.argv) <= 1:\n",
    "\tprint('Usage: %s <train dataset> <validation dataset (optional)>' % sys.argv[0])\n",
    "\traise SystemExit\n",
    "\n",
    "train = np.loadtxt(sys.argv[1],delimiter=',', dtype=np.float32)\n",
    "\n",
    "valid = None\n",
    "if len(sys.argv)>=3:\n",
    "\tvalid = np.loadtxt(sys.argv[2],delimiter=',', dtype=np.float32)\t\t\n",
    "\tconcat = np.concatenate((train, valid))\n",
    "\tX = concat[:,:-1]\n",
    "\ty = concat[:,-1].astype(np.int) \n",
    "else:\n",
    "\tX = train[:,:-1]\n",
    "\ty = train[:,-1].astype(int)\n",
    "\n",
    "opf_us = OpfUS()\n",
    "X_res, y_res = opf_us.fit_resample(X, y, valid)\n",
    "\n",
    "path = 'data'\n",
    "common = COMMON()\n",
    "common.saveDataset(X_res, y_res, path, opf_us.__class__.__name__)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
