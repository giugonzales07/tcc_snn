{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd68cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\giueg\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.0\\\\python311.zip', 'C:\\\\Users\\\\giueg\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.0\\\\DLLs', 'C:\\\\Users\\\\giueg\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.0\\\\Lib', 'C:\\\\Users\\\\giueg\\\\.pyenv\\\\pyenv-win\\\\versions\\\\3.11.0', 'c:\\\\Users\\\\giueg\\\\.virtualenvs\\\\TCC_25-OGy0qkdu', '', 'c:\\\\Users\\\\giueg\\\\.virtualenvs\\\\TCC_25-OGy0qkdu\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\giueg\\\\.virtualenvs\\\\TCC_25-OGy0qkdu\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\giueg\\\\.virtualenvs\\\\TCC_25-OGy0qkdu\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\giueg\\\\.virtualenvs\\\\TCC_25-OGy0qkdu\\\\Lib\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\giueg\\\\OneDrive\\\\Documentos\\\\Workspace\\\\trab_Graduação\\\\TCC_25\\\\OPFlmb', 'c:\\\\Users\\\\giueg\\\\OneDrive\\\\Documentos\\\\Workspace\\\\trab_Graduação\\\\TCC_25']\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "#import kagglehub\n",
    "#from kagglehub import KaggleDatasetAdapter\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "import cv2 \n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "import argparse\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../../../TCC_25\"))  # Adiciona o diretório atual ao caminho\n",
    "print(sys.path)\n",
    "#sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'TCC_25', 'OPFlmb')))\n",
    "from OPFlmb.oversampling.o2pf import O2PF\n",
    "from OPFlmb.undersampling.opf_us import OpfUS\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1efc0a2",
   "metadata": {},
   "source": [
    "## Data over tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3807, 128, 128, 3)\n",
      "(3807,)\n",
      "(3045, 128, 128, 3)\n",
      "(762, 128, 128, 3)\n",
      "Distribuição original das classes: Counter({np.int64(0): 2435, np.int64(2): 402, np.int64(1): 208})\n",
      "Forma achatada: (3045, 49152)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 18.0 GiB for an array with shape (49152, 49152) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m k_max = \u001b[32m5\u001b[39m  \u001b[38;5;66;03m# Escolha um valor para k_max (exemplo: 5, conforme sugerido no repositório)\u001b[39;00m\n\u001b[32m     57\u001b[39m o2pf = O2PF(k_max=k_max)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m x_res, y_res = \u001b[43mo2pf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Reformatar os dados para o formato de imagem\u001b[39;00m\n\u001b[32m     61\u001b[39m x_res = x_res.reshape(-\u001b[32m1\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giueg\\OneDrive\\Documentos\\Workspace\\trab_Graduação\\TCC_25\\OPFlmb\\oversampling\\os.py:154\u001b[39m, in \u001b[36mOS.fit_resample\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.min_class_label = uniques[\u001b[32m0\u001b[39m][sorted_un[i]]\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Generate the synthetic samples\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     synth_x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_new_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m     x_max, y_max = \u001b[38;5;28mself\u001b[39m.concatenate(x_min, y_min, x_max, y_max, synth_x, \u001b[38;5;28mself\u001b[39m.min_class_label)\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x_max,y_max\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giueg\\OneDrive\\Documentos\\Workspace\\trab_Graduação\\TCC_25\\OPFlmb\\oversampling\\o2pf.py:18\u001b[39m, in \u001b[36mO2PF.variant\u001b[39m\u001b[34m(self, X, generate_n, max_k)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvariant\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, generate_n, max_k):\n\u001b[32m     17\u001b[39m \tclf, cluster2samples = \u001b[38;5;28mself\u001b[39m.run(X, max_k)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcomputeVariant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster2samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43mestimation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean_gaussian\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43msampling\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giueg\\OneDrive\\Documentos\\Workspace\\trab_Graduação\\TCC_25\\OPFlmb\\oversampling\\os.py:80\u001b[39m, in \u001b[36mOS.computeVariant\u001b[39m\u001b[34m(self, clf, cluster2samples, X, generate_n, estimation_fn, sampling_fn)\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sample_ids) < CLUSTER_MIN_SIZE:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     mean, cova = \u001b[43mestimation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     gaussian_params.append(GaussianParams(\n\u001b[32m     82\u001b[39m         sample_ids,\n\u001b[32m     83\u001b[39m         mean,\n\u001b[32m     84\u001b[39m         cova,\n\u001b[32m     85\u001b[39m     ))\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Consider only samples in valid clusters to compute fraction of samples\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# per cluster, else the number of generated samples may be smaller than\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# `generate_n`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giueg\\OneDrive\\Documentos\\Workspace\\trab_Graduação\\TCC_25\\OPFlmb\\oversampling\\core\\estimation.py:31\u001b[39m, in \u001b[36mmean_gaussian\u001b[39m\u001b[34m(X, sample_ids, subgraph)\u001b[39m\n\u001b[32m     28\u001b[39m cluster_X = X[sample_ids]\n\u001b[32m     30\u001b[39m cluster_mean = cluster_X.mean(axis=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m cluster_cova = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowvar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cluster_mean, cluster_cova\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\giueg\\.virtualenvs\\TCC_25-OGy0qkdu\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:2893\u001b[39m, in \u001b[36mcov\u001b[39m\u001b[34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[39m\n\u001b[32m   2891\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2892\u001b[39m     X_T = (X*w).T\n\u001b[32m-> \u001b[39m\u001b[32m2893\u001b[39m c = \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_T\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2894\u001b[39m c *= np.true_divide(\u001b[32m1\u001b[39m, fact)\n\u001b[32m   2895\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m c.squeeze()\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 18.0 GiB for an array with shape (49152, 49152) and data type float64"
     ]
    }
   ],
   "source": [
    "# Tentativa oversampling O2PF\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Load the data(images)\n",
    "x_npz = np.load(\"../../data/x_images_arrays.npz\")\n",
    "X = x_npz[\"arr_0\"]\n",
    "y_npz = np.load(\"../../data/y_labels_arrays.npz\")\n",
    "Y = y_npz[\"arr_0\"]\n",
    "\n",
    "print(np.array(X).shape)\n",
    "print(np.array(Y).shape)\n",
    "\n",
    "# Separete the data into train and test sets\n",
    "# 80% train, 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=1, stratify=Y)\n",
    "\n",
    "#x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "print(np.array(x_train).shape)\n",
    "#print(np.array(x_val).shape)\n",
    "print(np.array(x_test).shape)\n",
    "# (3045, 128, 128, 3)\n",
    "# (0, 128, 128, 3)\n",
    "# (762, 128, 128, 3)\n",
    "\n",
    "# Transform the data to tensor\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            #transforms.Resize((28, 28)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# Verificar distribuição das classes\n",
    "print(\"Distribuição original das classes:\", Counter(y_train))\n",
    "\n",
    "# Achatamento das imagens para O2PF (necessário para formato 2D)\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)  # Forma: (3045, 128*128*3)\n",
    "print(\"Forma achatada:\", x_train_flat.shape)\n",
    "\n",
    "# Aplicar SMOTE\n",
    "\n",
    "# Aplicar O2PF\n",
    "k_max = 5  # Escolha um valor para k_max (exemplo: 5, conforme sugerido no repositório)\n",
    "o2pf = O2PF(k_max=k_max)\n",
    "x_res, y_res = o2pf.fit_resample(x_train_flat, y_train)\n",
    "\n",
    "# Reformatar os dados para o formato de imagem\n",
    "x_res = x_res.reshape(-1, 128, 128, 3)\n",
    "print(\"Forma após reamostragem:\", x_res.shape)\n",
    "print(\"Distribuição após reamostragem:\", Counter(y_res))\n",
    "\n",
    "# Apply the transform to the data\n",
    "dataset_train_res = CustomImageDataset(x_res, y_res, transform=transform)\n",
    "#dataset_val = CustomImageDataset(x_val, y_val, transform=transform)\n",
    "dataset_test = CustomImageDataset(x_test, y_test, transform=transform)\n",
    "\n",
    "img, label = dataset_train_res[0]\n",
    "print(img.shape, label) # [grayscale=1, size=128, size=128] label=0 ('Nothing')\n",
    "print(dataset_train_res.__len__()) # 3045 images\n",
    "#print(dataset_val.__len__()) # 0 images\n",
    "print(dataset_test.__len__()) # 762 images\n",
    "\n",
    "# Load into the DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset_train_res, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37415c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  (3807, 128, 128, 3)\n",
      "Y =  (3807,)\n",
      "\n",
      "Forma dos dados de treino: (3045, 128, 128, 3)\n",
      "Forma dos dados de teste: (762, 128, 128, 3)\n",
      "\n",
      "Distribuição original das classes train: Counter({np.int64(0): 2435, np.int64(2): 402, np.int64(1): 208})\n",
      "Distribuição original das classes train_adj: Counter({np.int64(1): 2435, np.int64(3): 402, np.int64(2): 208})\n",
      "\n",
      "Forma achatada: (3045, 49152)\n",
      "\n",
      "Forma antes reamostragem: (610, 49152)\n",
      "Distribuição antes reamostragem: Counter({np.int64(3): 402, np.int64(2): 208})\n",
      "\n",
      "Forma após reamostragem: (610, 49152)\n",
      "Distribuição após reamostragem: Counter({np.int64(2): 402, np.int64(1): 208})\n",
      "\n",
      "Forma do img e label: torch.Size([1, 128, 128]) 2\n",
      "Quantidade dados de treino: 610\n",
      "Quantidade dados de teste: 762\n"
     ]
    }
   ],
   "source": [
    "# Tentativa undersampling O2PF\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Load the data(images)\n",
    "x_npz = np.load(\"../../data/x_images_arrays.npz\")\n",
    "X = x_npz[\"arr_0\"]\n",
    "y_npz = np.load(\"../../data/y_labels_arrays.npz\")\n",
    "Y = y_npz[\"arr_0\"]\n",
    "\n",
    "print(\"x = \", np.array(X).shape)\n",
    "print(\"Y = \", np.array(Y).shape)\n",
    "\n",
    "# Separete the data into train and test sets\n",
    "# 80% train, 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=1, stratify=Y)\n",
    "\n",
    "#x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "print(\"\\nForma dos dados de treino:\", np.array(x_train).shape) # (3045, 128, 128, 3)\n",
    "#print(np.array(x_val).shape)\n",
    "print(\"Forma dos dados de teste:\", np.array(x_test).shape) # (762, 128, 128, 3)\n",
    "\n",
    "# Transform the data to tensor\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            #transforms.Resize((28, 28)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# Ajustar rótulos para o OpfUS (>= 1)\n",
    "y_train_adj = y_train + 1\n",
    "\n",
    "# Verificar distribuição das classes\n",
    "print(\"\\nDistribuição original das classes train:\", Counter(y_train))\n",
    "print(\"Distribuição original das classes train_adj:\", Counter(y_train_adj))\n",
    "\n",
    "# Achatamento das imagens para O2PF (necessário para formato 2D)\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)  # Forma: (3045, 128*128*3)\n",
    "print(\"\\nForma achatada:\", x_train_flat.shape)\n",
    "\n",
    "# Separar as classes para processar \n",
    "\n",
    "# Aplicar OpfUS\n",
    "opf_us = OpfUS()\n",
    "x_res, y_res = opf_us.fit_resample(x_train_flat, y_train_adj)\n",
    "\n",
    "print(\"\\nForma antes reamostragem:\", x_res.shape)\n",
    "print(\"Distribuição antes reamostragem:\", Counter(y_res))\n",
    "\n",
    "# Corrigir os rótulos de volta para começar em 0 (como o resto da pipeline espera)\n",
    "y_res_new = y_res - 1\n",
    "\n",
    "# Reformatar os dados para o formato de imagem\n",
    "x_res_new = x_res.reshape(-1, 128, 128, 3)\n",
    "print(\"\\nForma após reamostragem:\", x_res.shape)\n",
    "print(\"Distribuição após reamostragem:\", Counter(y_res_new))\n",
    "\n",
    "# Apply the transform to the data\n",
    "dataset_train_res = CustomImageDataset(x_res_new, y_res_new, transform=transform)\n",
    "#dataset_val = CustomImageDataset(x_val, y_val, transform=transform)\n",
    "dataset_test = CustomImageDataset(x_test, y_test, transform=transform)\n",
    "\n",
    "img, label = dataset_train_res[0]\n",
    "print(\"\\nForma do img e label:\", img.shape, label) # [grayscale=1, size=128, size=128] label=0 ('Nothing')\n",
    "print(\"Quantidade dados de treino:\", dataset_train_res.__len__()) # 3045 images\n",
    "#print(dataset_val.__len__()) # 0 images\n",
    "print(\"Quantidade dados de teste:\", dataset_test.__len__()) # 762 images\n",
    "\n",
    "# Load into the DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset_train_res, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc19a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_adj = y_test + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecc6d728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n",
      "Rótulos de teste ajustados: (762, 128, 128, 3)\n",
      "Distribuição rótulos de teste ajustados (adj): Counter({np.int64(1): 609, np.int64(3): 101, np.int64(2): 52})\n",
      "Distribuição rótulos de teste ajustados : Counter({np.int64(0): 609, np.int64(2): 101, np.int64(1): 52})\n",
      "TRAIN\n",
      "Rótulos de teste ajustados: (610, 128, 128, 3)\n",
      "Distribuição rótulos de teste ajustados (new): Counter({np.int64(2): 402, np.int64(1): 208})\n",
      "Distribuição rótulos de teste ajustados: Counter({np.int64(3): 402, np.int64(2): 208})\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST\")\n",
    "print(\"Rótulos de teste ajustados:\", x_test.shape)\n",
    "print(\"Distribuição rótulos de teste ajustados (adj):\", Counter(y_test_adj))\n",
    "print(\"Distribuição rótulos de teste ajustados :\", Counter(y_test))\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(\"Rótulos de teste ajustados:\", x_res_new.shape)\n",
    "print(\"Distribuição rótulos de teste ajustados (new):\", Counter(y_res_new))\n",
    "print(\"Distribuição rótulos de teste ajustados:\", Counter(y_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvar novo dataset\n",
    "np.savez(\"../../data/x_images_arrays_under.npz\", arr_0=x_res)\n",
    "np.savez(\"../../data/y_labels_arrays_under.npz\", arr_0=y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  (610, 128, 128, 3)\n",
      "Y =  (610,)\n",
      "\n",
      "Forma dos dados de treino: (488, 128, 128, 3)\n",
      "Counter train: Counter({np.int64(2): 322, np.int64(1): 166})\n",
      "Forma dos dados de teste: (122, 128, 128, 3)\n",
      "Counter test: Counter({np.int64(2): 80, np.int64(1): 42})\n",
      "\n",
      "Forma do img e label: torch.Size([1, 128, 128]) 2\n",
      "Quantidade dados de treino: 488\n",
      "Quantidade dados de teste: 122\n"
     ]
    }
   ],
   "source": [
    "# Tentativa undersampling O2PF\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Load the data(images)\n",
    "x_npz_r = np.load(\"../../data/x_images_arrays_resampled.npz\")\n",
    "X = x_npz_r[\"arr_0\"]\n",
    "y_npz_r = np.load(\"../../data/y_labels_arrays_resampled.npz\")\n",
    "Y = y_npz_r[\"arr_0\"]\n",
    "\n",
    "print(\"x = \", np.array(X).shape)\n",
    "print(\"Y = \", np.array(Y).shape)\n",
    "\n",
    "# Separete the data into train and test sets\n",
    "# 80% train, 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=1, stratify=Y)\n",
    "#x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "print(\"\\nForma dos dados de treino:\", np.array(x_train).shape) # (3045, 128, 128, 3)\n",
    "print(\"Counter train:\", Counter(y_train))\n",
    "#print(np.array(x_val).shape)\n",
    "print(\"Forma dos dados de teste:\", np.array(x_test).shape) # (762, 128, 128, 3)\n",
    "print(\"Counter test:\", Counter(y_test))\n",
    "\n",
    "# Transform the data to tensor\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            #transforms.Resize((28, 28)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# Apply the transform to the data\n",
    "dataset_train = CustomImageDataset(x_train, y_train, transform=transform)\n",
    "#dataset_val = CustomImageDataset(x_val, y_val, transform=transform)\n",
    "dataset_test = CustomImageDataset(x_test, y_test, transform=transform)\n",
    "\n",
    "img, label = dataset_train[0]\n",
    "print(\"\\nForma do img e label:\", img.shape, label) # [grayscale=1, size=128, size=128] label=0 ('Nothing')\n",
    "print(\"Quantidade dados de treino:\", dataset_train.__len__()) # 3045 images\n",
    "#print(dataset_val.__len__()) # 0 images\n",
    "print(\"Quantidade dados de teste:\", dataset_test.__len__()) # 762 images\n",
    "\n",
    "# Load into the DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e315e2",
   "metadata": {},
   "source": [
    "## Under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  (3807, 128, 128, 3)\n",
      "Y =  (3807,)\n",
      "\n",
      "Forma dos dados de treino: (3045, 128, 128, 3)\n",
      "Distribuição train antes: Counter({np.int64(0): 2435, np.int64(2): 402, np.int64(1): 208})\n",
      "Forma dos dados de teste: (762, 128, 128, 3)\n",
      "Distribuição test antes: Counter({np.int64(0): 609, np.int64(2): 101, np.int64(1): 52})\n",
      "\n",
      "Forma da trian após reamostragem: (1012, 128, 128, 3)\n",
      "Distribuição train após reamostragem: Counter({np.int64(2): 402, np.int64(0): 402, np.int64(1): 208})\n"
     ]
    }
   ],
   "source": [
    "# Tentativa undersampling O2PF\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Load the data(images)\n",
    "x_npz = np.load(\"../../data/x_images_arrays.npz\")\n",
    "X = x_npz[\"arr_0\"]\n",
    "y_npz = np.load(\"../../data/y_labels_arrays.npz\")\n",
    "Y = y_npz[\"arr_0\"]\n",
    "\n",
    "print(\"x = \", np.array(X).shape)\n",
    "print(\"Y = \", np.array(Y).shape)\n",
    "\n",
    "# Separete the data into train and test sets\n",
    "# 80% train, 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=1, stratify=Y)\n",
    "\n",
    "#x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "print(\"\\nForma dos dados de treino:\", np.array(x_train).shape) # (3045, 128, 128, 3)\n",
    "print(\"Distribuição train antes:\", Counter(y_train)) #np.int64(0): 2435, np.int64(2): 402, np.int64(1): 208\n",
    "#print(np.array(x_val).shape)\n",
    "print(\"Forma dos dados de teste:\", np.array(x_test).shape) # (762, 128, 128, 3)\n",
    "print(\"Distribuição test antes:\", Counter(y_test)) #np.int64(0): 609, np.int64(2): 101, np.int64(1): 52\n",
    "\n",
    "# Transform the data to tensor\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            #transforms.Resize((28, 28)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# Índices de cada classe\n",
    "idx_class_0 = np.where(y_train == 0)[0]\n",
    "idx_class_1 = np.where(y_train == 1)[0]\n",
    "idx_class_2 = np.where(y_train == 2)[0]\n",
    "\n",
    "# Undersample da classe 1 para ficar com, por exemplo, o mesmo número da classe 3\n",
    "n_samples = len(idx_class_2)  # ou defina outro número\n",
    "np.random.seed(42)\n",
    "idx_class_0_sampled = np.random.choice(idx_class_0, size=n_samples, replace=False)\n",
    "\n",
    "# Combinar os índices finais\n",
    "final_indices = np.concatenate([idx_class_0_sampled, idx_class_1, idx_class_2])\n",
    "np.random.shuffle(final_indices)\n",
    "\n",
    "# Aplicar seleção nos dados\n",
    "x_train_resampled = x_train[final_indices]\n",
    "y_train_resampled = y_train[final_indices]\n",
    "\n",
    "print(\"\\nForma da trian após reamostragem:\", x_train_resampled.shape) #(1012, 128, 128, 3)\n",
    "print(\"Distribuição train após reamostragem:\", Counter(y_train_resampled)) #np.int64(2): 402, np.int64(0): 402, np.int64(1): 208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78764002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forma do img e label: torch.Size([1, 128, 128]) 1\n",
      "Quantidade dados de treino: 610\n",
      "Quantidade dados de teste: 762\n"
     ]
    }
   ],
   "source": [
    "# Apply the transform to the data\n",
    "dataset_train_res = CustomImageDataset(x_res_new, y_train_resampled, transform=transform)\n",
    "#dataset_val = CustomImageDataset(x_val, y_val, transform=transform)\n",
    "dataset_test = CustomImageDataset(x_test, y_test, transform=transform)\n",
    "\n",
    "img, label = dataset_train_res[0]\n",
    "print(\"\\nForma do img e label:\", img.shape, label) # [grayscale=1, size=128, size=128] label=0 ('Nothing')\n",
    "print(\"Quantidade dados de treino:\", dataset_train_res.__len__()) # 610 images\n",
    "#print(dataset_val.__len__()) # 0 images\n",
    "print(\"Quantidade dados de teste:\", dataset_test.__len__()) # 762 images\n",
    "\n",
    "# Load into the DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset_train_res, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d497ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvar novo dataset\n",
    "np.savez(\"../../data/x_images_arrays_under_train.npz\", arr_0=x_train_resampled)\n",
    "np.savez(\"../../data/y_labels_arrays_under_train.npz\", arr_0=y_train_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1eded7",
   "metadata": {},
   "source": [
    "## Over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e02bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentativa oversampling O2PF\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Load the data(images)\n",
    "x_npz = np.load(\"../../data/x_images_arrays.npz\")\n",
    "X = x_npz[\"arr_0\"]\n",
    "y_npz = np.load(\"../../data/y_labels_arrays.npz\")\n",
    "Y = y_npz[\"arr_0\"]\n",
    "\n",
    "print(np.array(X).shape)\n",
    "print(np.array(Y).shape)\n",
    "\n",
    "# Separete the data into train and test sets\n",
    "# 80% train, 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=1, stratify=Y)\n",
    "\n",
    "#x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "print(np.array(x_train).shape)\n",
    "#print(np.array(x_val).shape)\n",
    "print(np.array(x_test).shape)\n",
    "# (3045, 128, 128, 3)\n",
    "# (0, 128, 128, 3)\n",
    "# (762, 128, 128, 3)\n",
    "\n",
    "# Transform the data to tensor\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            #transforms.Resize((28, 28)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# Verificar distribuição das classes\n",
    "print(\"Distribuição original das classes:\", Counter(y_train))\n",
    "\n",
    "# Achatamento das imagens para O2PF (necessário para formato 2D)\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)  # Forma: (3045, 128*128*3)\n",
    "print(\"Forma achatada:\", x_train_flat.shape)\n",
    "\n",
    "# Aplicar SMOTE\n",
    "\n",
    "# Aplicar O2PF\n",
    "k_max = 5  # Escolha um valor para k_max (exemplo: 5, conforme sugerido no repositório)\n",
    "o2pf = O2PF(k_max=k_max)\n",
    "x_res, y_res = o2pf.fit_resample(x_train_flat, y_train)\n",
    "\n",
    "# Reformatar os dados para o formato de imagem\n",
    "x_res = x_res.reshape(-1, 128, 128, 3)\n",
    "print(\"Forma após reamostragem:\", x_res.shape)\n",
    "print(\"Distribuição após reamostragem:\", Counter(y_res))\n",
    "\n",
    "# Apply the transform to the data\n",
    "dataset_train_res = CustomImageDataset(x_res, y_res, transform=transform)\n",
    "#dataset_val = CustomImageDataset(x_val, y_val, transform=transform)\n",
    "dataset_test = CustomImageDataset(x_test, y_test, transform=transform)\n",
    "\n",
    "img, label = dataset_train_res[0]\n",
    "print(img.shape, label) # [grayscale=1, size=128, size=128] label=0 ('Nothing')\n",
    "print(dataset_train_res.__len__()) # 3045 images\n",
    "#print(dataset_val.__len__()) # 0 images\n",
    "print(dataset_test.__len__()) # 762 images\n",
    "\n",
    "# Load into the DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset_train_res, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc456fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  (3807, 128, 128, 3)\n",
      "Y =  (3807,)\n",
      "\n",
      "Forma dos dados de treino: (1012, 128, 128, 3)\n",
      "Distribuição train antes: Counter({np.int64(2): 402, np.int64(0): 402, np.int64(1): 208})\n",
      "Forma dos dados de teste: (762, 128, 128, 3)\n",
      "Distribuição test antes: Counter({np.int64(0): 609, np.int64(2): 101, np.int64(1): 52})\n",
      "Forma achatada: (1012, 49152)\n",
      "Distribuição da classe 1 após O2PF: Counter({np.int64(1): 208})\n",
      "Distribuição da classe 1 após ajuste: Counter({np.int64(1): 402})\n",
      "\n",
      "Forma dos dados de treino: (1206, 128, 128, 3)\n",
      "Distribuição train antes: Counter({np.int64(2): 402, np.int64(0): 402, np.int64(1): 402})\n",
      "Forma dos dados de teste: (762, 128, 128, 3)\n",
      "Distribuição test antes: Counter({np.int64(0): 609, np.int64(2): 101, np.int64(1): 52})\n",
      "\n",
      "Forma do img e label: torch.Size([1, 128, 128]) 2\n",
      "Quantidade dados de treino: 1206\n",
      "Quantidade dados de teste: 762\n"
     ]
    }
   ],
   "source": [
    "# Tentativa oversampling O2PF depois do undersampling\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Load the data(images)\n",
    "x_npz = np.load(\"../../data/x_images_arrays.npz\")\n",
    "X = x_npz[\"arr_0\"]\n",
    "y_npz = np.load(\"../../data/y_labels_arrays.npz\")\n",
    "Y = y_npz[\"arr_0\"]\n",
    "\n",
    "print(\"x = \", np.array(X).shape)\n",
    "print(\"Y = \", np.array(Y).shape)\n",
    "\n",
    "# Separete the data into train and test sets\n",
    "# 80% train, 20% test\n",
    "x_train_old, x_test, y_train_old, y_test = train_test_split(X, Y, test_size=0.2,random_state=1, stratify=Y)\n",
    "\n",
    "x_train_aux = np.load(\"../../data/x_images_arrays_under_train.npz\")\n",
    "x_train = x_train_aux[\"arr_0\"]\n",
    "y_train_aux = np.load(\"../../data/y_labels_arrays_under_train.npz\")\n",
    "y_train = y_train_aux[\"arr_0\"]\n",
    "\n",
    "#x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "print(\"\\nForma dos dados de treino:\", np.array(x_train).shape) # (1012, 128, 128, 3)\n",
    "print(\"Distribuição train antes:\", Counter(y_train)) #{np.int64(2): 402, np.int64(0): 402, np.int64(1): 208}\n",
    "#print(np.array(x_val).shape)\n",
    "print(\"Forma dos dados de teste:\", np.array(x_test).shape) # (762, 128, 128, 3)\n",
    "print(\"Distribuição test antes:\", Counter(y_test)) #np.int64(0): 609, np.int64(2): 101, np.int64(1): 52\n",
    "\n",
    "# Transform the data to tensor\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            #transforms.Resize((28, 28)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# # Achatamento das imagens para O2PF (necessário para formato 2D)\n",
    "# x_train_flat = x_train.reshape(x_train.shape[0], -1)  # Forma: (1012, 128*128*3)\n",
    "# print(\"Forma achatada:\", x_train_flat.shape)\n",
    "\n",
    "# # Aplicar O2PF\n",
    "# k_max = 5  # Escolha um valor para k_max (exemplo: 5, conforme sugerido no repositório)\n",
    "# o2pf = O2PF(k_max=k_max)\n",
    "# x_res, y_train_res = o2pf.fit_resample(x_train_flat, y_train)\n",
    "\n",
    "# Achatamento das imagens para O2PF (necessário para formato 2D)\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)  # Forma: (1012, 128*128*3)\n",
    "print(\"Forma achatada:\", x_train_flat.shape)\n",
    "\n",
    "# Filtrar apenas a classe 1 (208 amostras)\n",
    "mask_class1 = y_train == 1\n",
    "x_train_class1 = x_train_flat[mask_class1]  # Forma: (208, 128*128*3)\n",
    "y_train_class1 = y_train[mask_class1]  # Forma: (208,)\n",
    "\n",
    "# Aplicar O2PF apenas à classe 1\n",
    "k_max = 5  # Ajuste conforme necessário\n",
    "o2pf = O2PF(k_max=k_max)\n",
    "x_res_class1, y_res_class1 = o2pf.fit_resample(x_train_class1, y_train_class1)\n",
    "\n",
    "# Verificar a nova distribuição da classe 1\n",
    "print(\"Distribuição da classe 1 após O2PF:\", Counter(y_res_class1))\n",
    "\n",
    "# Ajustar manualmente para 402 amostras, se necessário\n",
    "desired_samples = 402\n",
    "current_samples = len(y_res_class1)\n",
    "if current_samples > desired_samples:\n",
    "    # Reduzir para 402 amostras (seleção aleatória)\n",
    "    indices = np.random.choice(current_samples, desired_samples, replace=False)\n",
    "    x_res_class1 = x_res_class1[indices]\n",
    "    y_res_class1 = y_res_class1[indices]\n",
    "elif current_samples < desired_samples:\n",
    "    # Aumentar com duplicação aleatória, se necessário\n",
    "    additional_samples = desired_samples - current_samples\n",
    "    indices = np.random.choice(current_samples, additional_samples, replace=True)\n",
    "    x_res_class1 = np.concatenate([x_res_class1, x_res_class1[indices]], axis=0)\n",
    "    y_res_class1 = np.concatenate([y_res_class1, y_res_class1[indices]], axis=0)\n",
    "\n",
    "print(\"Distribuição da classe 1 após ajuste:\", Counter(y_res_class1))\n",
    "\n",
    "# Combinar os dados reamostrados da classe 1 com as classes 0 e 2\n",
    "x_train_other = x_train_flat[~mask_class1]  # Classes 0 e 2\n",
    "y_train_other = y_train[~mask_class1]\n",
    "x_res = np.concatenate([x_train_other, x_res_class1], axis=0)\n",
    "y_train_res = np.concatenate([y_train_other, y_res_class1], axis=0)\n",
    "\n",
    "# Reformatar os dados para o formato de imagem\n",
    "x_train_res = x_res.reshape(-1, 128, 128, 3)\n",
    "\n",
    "print(\"\\nForma dos dados de treino:\", np.array(x_train_res).shape) # \n",
    "print(\"Distribuição train antes:\", Counter(y_train_res)) #\n",
    "#print(np.array(x_val).shape)\n",
    "print(\"Forma dos dados de teste:\", np.array(x_test).shape) # (762, 128, 128, 3)\n",
    "print(\"Distribuição test antes:\", Counter(y_test)) #np.int64(0): 609, np.int64(2): 101, np.int64(1): 52\n",
    "\n",
    "# Apply the transform to the data\n",
    "dataset_train = CustomImageDataset(x_train_res, y_train_res, transform=transform)\n",
    "#dataset_val = CustomImageDataset(x_val, y_val, transform=transform)\n",
    "dataset_test = CustomImageDataset(x_test, y_test, transform=transform)\n",
    "\n",
    "img, label = dataset_train[0]\n",
    "print(\"\\nForma do img e label:\", img.shape, label) # [grayscale=1, size=128, size=128] label=0 ('Nothing')\n",
    "print(\"Quantidade dados de treino:\", dataset_train.__len__()) # 610 images\n",
    "#print(dataset_val.__len__()) # 0 images\n",
    "print(\"Quantidade dados de teste:\", dataset_test.__len__()) # 762 images\n",
    "\n",
    "# Load into the DataLoader\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3388533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvar novo dataset\n",
    "np.savez(\"../../data/x_images_arrays_under_train_over.npz\", arr_0=x_train_res)\n",
    "np.savez(\"../../data/y_labels_arrays_under_train_over.npz\", arr_0=y_train_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCC_25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
