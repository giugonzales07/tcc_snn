{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce5344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "import cv2 \n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "import argparse\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e51ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3807, 128, 128, 3)\n",
      "(3807,)\n",
      "(3045, 128, 128, 3)\n",
      "(381, 128, 128, 3)\n",
      "(381, 128, 128, 3)\n",
      "(401, 128, 128, 3)\n",
      "(401,)\n",
      "torch.Size([1, 128, 128]) 0\n",
      "401\n",
      "381\n",
      "381\n"
     ]
    }
   ],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label \n",
    "\n",
    "# Load the data(images)\n",
    "x_npz = np.load(\"data/x_images_arrays.npz\")\n",
    "X = x_npz[\"arr_0\"]\n",
    "y_npz = np.load(\"data/y_labels_arrays.npz\")\n",
    "Y = y_npz[\"arr_0\"]\n",
    "\n",
    "print(np.array(X).shape)\n",
    "print(np.array(Y).shape)\n",
    "\n",
    "# Separete the data into train, val and test sets\n",
    "# 80% train, 10% val, 10% test\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.2,random_state=1, stratify=Y)\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "print(np.array(x_train).shape)\n",
    "print(np.array(x_val).shape)\n",
    "print(np.array(x_test).shape)\n",
    "# (3045, 128, 128, 3)\n",
    "# (381, 128, 128, 3)\n",
    "# (381, 128, 128, 3)\n",
    "\n",
    "# Reduce the training dataset size to 401 images\n",
    "x_train_subset = x_train[:401]  # Select first 401 images\n",
    "y_train_subset = y_train[:401]  # Select corresponding labels\n",
    "print(np.array(x_train_subset).shape)  # (401, 128, 128, 3)\n",
    "print(np.array(y_train_subset).shape)  # (401,)\n",
    "\n",
    "# Transform the data to tensor\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            #transforms.Resize((28, 28)),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# Apply the transform to the data\n",
    "dataset_train_under = CustomImageDataset(x_train_subset, y_train_subset, transform=transform)\n",
    "dataset_val = CustomImageDataset(x_val, y_val, transform=transform)\n",
    "dataset_test = CustomImageDataset(x_val, y_val, transform=transform)\n",
    "\n",
    "img, label = dataset_train_under[0]\n",
    "print(img.shape, label) # [grayscale=1, size=128, size=128] label=0 ('Nothing')\n",
    "print(dataset_train_under.__len__()) # 3045 images\n",
    "print(dataset_val.__len__()) # 381 images\n",
    "print(dataset_test.__len__()) # 381 images\n",
    "\n",
    "# Load into the DataLoader\n",
    "# batch_size = 32\n",
    "\n",
    "# train_loader = DataLoader(dataset_train_under, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8281ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar os dados balanceados em um arquivo .npz\n",
    "np.savez('data/train_subset.npz', images=x_train_subset, labels=y_train_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCC_25-OGy0qkdu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
